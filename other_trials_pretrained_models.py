# -*- coding: utf-8 -*-
"""Other trials - Pretrained models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z4RzdJfTtXX6AdPM4TMWV1Yn_rKxwb9z
"""

#To mount drive run this command
from google.colab import drive
drive.mount('/content/drive')

"""##Import the Libraries, using TensorFlow backend"""

#import all library
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import random
import cv2
import io
from PIL import Image
from IPython.display import display,clear_output
from warnings import filterwarnings
from sklearn.metrics import classification_report
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Layer, Flatten, GlobalMaxPool2D
from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization
from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB3
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau



editor='giacomo'



if editor == 'giacomo':
  path_training='/content/drive/MyDrive/Progetto_VISIOPE/Img/Training'
  path_testing='/content/drive/MyDrive/Progetto_VISIOPE/Img/Testing'


elif editor=='francesco':
  path_training='/content/drive/MyDrive/Project_VP/archive/Training'
  path_testing='/content/drive/MyDrive/Project_VP/archive/Testing'



print('Libraries imported')
print("Hello", editor, "have fun working on this project","\n", "Your TRAINING path is ", path_training, "\n", "Your TESTING path is ", path_testing)

"""## Plot dataset distribution"""

# Get Class Names
class_names = sorted(os.listdir(path_training))
n_classes = len(class_names)
print(f"Class Names: \n{class_names}")
print(f"Total Number of Classes : {n_classes}")

class_dis = [len(os.listdir(path_training + f"/{name}")) for name in class_names]
print(f"Class Distribution : \n{class_dis}")

import plotly.express as px
fig = px.pie(names=class_names, values=class_dis, width=600)
fig.update_layout({"title":{'text':"Class Distribution","x":0.5}})
fig.show()

import seaborn as sns

plt.figure(figsize=(10,5))
sns.barplot(x=class_names, y=class_dis)
plt.grid()
plt.axhline(np.mean(class_dis), color='k', linestyle='--', label="Mean Images")
plt.legend()
plt.show()

"""##Plot Images of the dataset"""

data = {}
for root, _, filenames in os.walk(path_training):
    dirname = root.split('/')[-1]

    for filename in filenames:

        if dirname not in data.keys():
            data[dirname] = []

        data[dirname].append(os.path.join(root, filename))

"""Plot Random Samples from each class"""

plt.figure(figsize=(20, 20))

samples = []
for i, k in enumerate(data.keys()):
    s = cv2.imread(random.choice(data[k]))
    s = cv2.cvtColor(s, cv2.COLOR_BGR2GRAY)
    samples.append(s)

    plt.subplot(1, 4, i + 1)
    plt.imshow(samples[-1], cmap='gray')
    plt.title(k + ' sample')
    plt.xticks([])
    plt.yticks([])

"""Grayscale hystogram of the images behind"""

plt.figure(figsize=(30, 5))

for i, s in enumerate(samples):
    plt.subplot(1, 4, i + 1)
    plt.hist(s.ravel(),256,[0,256])
    plt.title(list(data.keys())[i] + ' sample')

"""##Basic processing on the images

*   Canny



"""

plt.figure(figsize=(15, 15))

for i, s in enumerate(samples):
    s = cv2.Canny(s, threshold1=140, threshold2=210)

    plt.subplot(1, 4, i + 1)
    plt.imshow(s, cmap='gray')
    plt.title(list(data.keys())[i] + ' sample')
    plt.xticks([])
    plt.yticks([])

"""*   Sobel X -> to search the the gradient of the image in the horizontal axis
*   Sobel Y -> to search the the gradient of the image in the vertical axis
*   Laplacian
*   Blended
"""

plt.figure(figsize=(15, 15))

j = 0
for i, s in enumerate(samples):
    sobelx = cv2.Sobel(s, cv2.CV_64F, 1, 0, ksize=5)
    sobely = cv2.Sobel(s, cv2.CV_64F, 0, 1, ksize=5)
    laplacian = cv2.Laplacian(s, cv2.CV_64F)
    blended = cv2.addWeighted(src1=sobelx, alpha=0.5, src2=sobely, beta=0.5,gamma=0)

    plt.subplot(4, 4, i + j + 1)
    plt.imshow(sobelx, cmap='gray')
    plt.title(list(data.keys())[i] + ' sample sobelx')
    plt.xticks([])
    plt.yticks([])

    plt.subplot(4, 4, i + j + 2)
    plt.imshow(sobely, cmap='gray')
    plt.title(list(data.keys())[i] + ' sample sobely')
    plt.xticks([])
    plt.yticks([])

    plt.subplot(4, 4, i + j + 3)
    plt.imshow(laplacian, cmap='gray')
    plt.title(list(data.keys())[i] + ' sample laplacian')
    plt.xticks([])
    plt.yticks([])

    plt.subplot(4, 4, i + j + 4)
    plt.imshow(blended, cmap='gray')
    plt.title(list(data.keys())[i] + ' sample blended')
    plt.xticks([])
    plt.yticks([])

    j += 3

"""## Data Generator (model 1)

Generates batches of augmented data and Split data set
"""

img_size = 150
batch_size = 32

generator_train = ImageDataGenerator(rescale=1./255,
                                    rotation_range=20,
                                    width_shift_range=0.1,
                                    height_shift_range=0.1,
                                    shear_range=0.1,
                                    zoom_range=0.1,
                                    horizontal_flip=True,
                                    vertical_flip=True,
                                    fill_mode="nearest")

generator_test = ImageDataGenerator(rescale=1./255)



train = generator_train.flow_from_directory(path_training,
                                            target_size=(img_size, img_size),
                                            batch_size=batch_size,
                                            class_mode='categorical',
                                            )

test = generator_test.flow_from_directory(path_testing,
                                          target_size=(img_size, img_size),
                                          batch_size=batch_size,
                                          class_mode='categorical',
                                          )

"""## EfficientNetB0 Model"""



#EfficientNet TRAIN --> INPUT SIZE 224,224,3 NO GRAYSCALE


img_size = 150
batch_size = 32

generator_train = ImageDataGenerator(rescale=1./255,
                                    rotation_range=20,
                                    width_shift_range=0.1,
                                    height_shift_range=0.1,
                                    shear_range=0.1,
                                    zoom_range=0.1,
                                    horizontal_flip=True,
                                    vertical_flip=True,
                                    fill_mode="nearest")

generator_test = ImageDataGenerator(rescale=1./255)



train = generator_train.flow_from_directory(path_training,
                                            target_size=(img_size, img_size),
                                            batch_size=batch_size,
                                            class_mode='categorical',
                                            )

test = generator_test.flow_from_directory(path_testing,
                                          target_size=(img_size, img_size),
                                          batch_size=batch_size,
                                          class_mode='categorical',
                                          )

from tensorflow.keras.applications import EfficientNetB0
eff_model = EfficientNetB0(input_shape=(150,150,3), include_top=False, weights='imagenet')

model = Sequential()
'''# Set all layers to non-trainable
for layer in eff_model.layers:
    layer.trainable = False


# Set the last vgg block to trainable

eff_model.layers[-2].trainable = True
eff_model.layers[-3].trainable = True

'''

model=eff_model.output
model=GlobalAveragePooling2D()(model)
model=Dropout(rate=0.4)(model)
model=Dense(4,activation='softmax')(model)
model=Model(inputs=eff_model.input,outputs=model)


model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])


model.summary()

"""## VGG Model

Convolutional Neural Network Model

VGG Train
"""

#VGG TRAIN --> INPUT SIZE 224,224,3 NO GRAYSCALE


img_size = 150
batch_size = 32

generator_train = ImageDataGenerator(rescale=1./255,
                                    rotation_range=20,
                                    width_shift_range=0.1,
                                    height_shift_range=0.1,
                                    shear_range=0.1,
                                    zoom_range=0.1,
                                    horizontal_flip=True,
                                    vertical_flip=True,
                                    fill_mode="nearest")

generator_test = ImageDataGenerator(rescale=1./255)



train = generator_train.flow_from_directory(path_training,
                                            target_size=(img_size, img_size),
                                            batch_size=batch_size,
                                            class_mode='categorical',
                                            )

test = generator_test.flow_from_directory(path_testing,
                                          target_size=(img_size, img_size),
                                          batch_size=batch_size,
                                          class_mode='categorical',
                                          )

from keras.applications.vgg16 import VGG16


vgg_model = VGG16(input_shape=(150,150,3), include_top=False, weights='imagenet')

model = Sequential()
# Set all layers to non-trainable
for layer in vgg_model.layers:
    layer.trainable = False



# Set the last vgg block to trainable

#vgg_model.layers[-2].trainable = True





#model.add(keras.Input(shape=(150,150,3)))
model.add(vgg_model)
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(4, activation='softmax'))




model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])


model.summary()

#Regularization
early_stopping = EarlyStopping(monitor = 'val_loss', restore_best_weights=True, patience = 5)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)
checkpoint = ModelCheckpoint(filepath = 'model_weights.h5', monitor = 'val_loss', save_best_only = True)

history = model.fit(train, steps_per_epoch=30, validation_data=test, epochs= 20, validation_steps= 10, callbacks=[early_stopping, reduce_lr, checkpoint])


#steps_per_epoch=5712/32
#validation_steps= 1311/32

"""Save model in the directory


*   model_1
*   model_2


"""

model.save('/content/drive/MyDrive/Progetto_VISIOPE/VGG_model')

"""Load the model"""

#load saved model
#model = tf.keras.models.load_model('/content/drive/MyDrive/Project_VP/model1.model')

"""##Results of the training"""

# Evaluate the model
loss, accuracy = model.evaluate(test, steps=1311/batch_size)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

"""Validation Accuracy"""

# Graphs of Training and Validation Accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'])
plt.show()

"""Validation Loss"""

# Graphs of Training and Validation Loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'])
plt.show()

"""SK LEARN classification

Predicts
"""

y_val = test.classes
y_pred = model.predict(test)
y_pred = np.argmax(y_pred,axis=1)

from sklearn.metrics import confusion_matrix, classification_report

print(classification_report(y_val,y_pred))

print(y_val)

"""##Evaluation

Model evaluation
"""

y_val = test.classes
y_pred = model.predict(test)
y_pred = np.argmax(y_pred,axis=1)

print(classification_report(y_val,y_pred))

class_indices = test.class_indices
indices = {v:k for k,v in class_indices.items()}

filenames = test.filenames

val_df = pd.DataFrame()
val_df['filename'] = filenames
val_df['actual'] = y_val
val_df['predicted'] = y_pred
val_df['actual'] = val_df['actual'].apply(lambda x: indices[x])
val_df['predicted'] = val_df['predicted'].apply(lambda x: indices[x])
val_df.loc[val_df['actual']==val_df['predicted'],'Same'] = True
val_df.loc[val_df['actual']!=val_df['predicted'],'Same'] = False
val_df.head(10)

val_df = val_df.sample(frac=1).reset_index(drop=True)

"""##Prediction

*   Correctly Classified (A and P are same)
*   Misclassified (A and P are different)
*   A: Actual P: Predicted
"""

def readImage(path):
    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))
    img = img_to_array(img)
    img = img/255.

    return img

def display_images(temp_df):
    temp_df = temp_df.reset_index(drop=True)
    plt.figure(figsize = (20 , 20))
    n = 0
    for i in range(15):
        n+=1
        plt.subplot(5 , 5, n)
        plt.subplots_adjust(hspace = 0.5 , wspace = 0.3)
        image = readImage(f"/content/drive/MyDrive/Progetto_VISIOPE/Img/Testing/{temp_df.filename[i]}")
        plt.imshow(image)
        plt.title(f'A: {temp_df.actual[i]} P: {temp_df.predicted[i]}')

"""Correctly classified"""

display_images(val_df[val_df['Same']==True])

"""Misclassified"""

display_images(val_df[val_df['Same']!=True])

"""###Confusion matrix

"""

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_true=y_val, y_pred=y_pred)

import itertools
def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    #else:
        #print('Confusion matrix, without normalization')
        #print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
        horizontalalignment="center",
        color="white" if cm[i, j] > thresh else "black")
        plt.tight_layout()
        plt.ylabel('True label')
        plt.xlabel('Predicted label')

cm_plot_labels = ['glioma', 'meningioma', 'notumor', 'pituitary']

plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')